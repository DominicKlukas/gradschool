---
tags:
  - technical_library
title: "Motion Prompting: Controlling Video Generation with Motion Trajectories"
authors: Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun
bibtex: "@misc{geng2025motionpromptingcontrollingvideo,      title={Motion Prompting: Controlling Video Generation with Motion Trajectories},       author={Daniel Geng and Charles Herrmann and Junhwa Hur and Forrester Cole and Serena Zhang and Tobias Pfaff and Tatiana Lopez-Guevara and Carl Doersch and Yusuf Aytar and Michael Rubinstein and Chen Sun and Oliver Wang and Andrew Owens and Deqing Sun},      year={2025},      eprint={2412.02700},      archivePrefix={arXiv},      primaryClass={cs.CV},      url={https://arxiv.org/abs/2412.02700}, }"
pretty_cite:
link: https://doi.org/10.48550/arXiv.2412.02700
topics: Generative AI
reading_lists:
projects:
type:
to_read: false
stars:
---
Use physical prompts (positions, times, velocities) as well as text prompts to generate videos.
Starts with a diffusion model: see [[2025_Holderrieth_FlowMatchingDiffusionModelsIntroduction]] to generate the video. ControlNet-like adapter. Not sure what this adapter is or how it works.