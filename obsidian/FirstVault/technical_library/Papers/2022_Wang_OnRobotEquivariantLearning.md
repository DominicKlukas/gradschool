---
tags:
  - technical_library
title: On-Robot Learning With Equivariant Models
authors: Dian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, Robert Platt
bibtex: "@misc{wang2022onrobotlearningequivariantmodels,      title={On-Robot Learning With Equivariant Models},       author={Dian Wang and Mingxi Jia and Xupeng Zhu and Robin Walters and Robert Platt},      year={2022},      eprint={2203.04923},      archivePrefix={arXiv},      primaryClass={cs.RO},      url={https://arxiv.org/abs/2203.04923}, }"
pretty_cite:
link: https://doi.org/10.48550/arXiv.2203.04923
topics: Equivariance, Robotics, RL, SAC
reading_lists:
projects: SDM Course Project
type:
to_read: true
stars:
---
Instead of using e(2), SO(2) or O(2), they show that they get better performance with discrete groups such as $D_4$, $C_8$, or $D_8$, along with data augmentation (a technique where you multiply your data by transforming it according to symmetries, or reuse your data in a clever way (buffer augmentation)) also performs significantly better than no equivariance.
In this paper, the tasks they learn perform so well with equivariance, that they no longer need sim2real... they learn fast enough on the real robot, and don't have to unlearn bad habits!

The neural networks used to train the SAC actor and critic are equivariant/invariant respectively to the group in question.

SAC: maximize entropy and reward on actor side using the Q function generated by the critic. The critic learns the Q function by computing the loss with the previously computed y values, which combine rewards with the Q function and entropy.