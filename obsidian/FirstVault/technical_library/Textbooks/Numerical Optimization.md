## Chapter 12: Theory of Constrained Optimization
- First, notice that strict local minimums don't require that there are no other local minimums nearby, just that they are higher... if we have an aggressively oscillating function down to a point, it can still have close local minima. This is why isolated local minima are stronger statements.
- Smoothness doesn't necessarily mean the feasible region has a smooth boundary... four affine constraints make a square, that has 4 non-smooth corners.
	- We can use this to make non-smooth unconstrained problems into smooth constrained problems (meaning both $f$, and $c_{i}$ are smooth).
- We derive Lagrange multipliers by showing that if the gradients of $f$ and $c$ are not parallel, there exists an infinitesimal vector $d$ such that when we move along $d$ then we reduce $f$ while staying on the constraint.
	- When we have two constraints, then our Lagrange multiplier becomes a vector with respect to the Lagrange multipliers of both of them (so we don't need to worry about being parallel to both constraints, a combination of them suffices for a "corner").
- We are seeking to define the tangent cone/constraint qualifications. The tangent cone defines the cone of directions we can reliably go in and stay in the constraint starting from a specific point. The linear feasible directions are generated using $\nabla c_{i}(x) = 0$ instead... we want this to be the same as the tangent cone... the problem though, is that it sometimes isn't. Consider $(x^{2}_{1} - x^{2}_{2} - 2)^{2} = 0$. This is quartic function, whose second derivative is zero everywhere on the cone... it stays "flat" locally, instead of increasing invariably as we deviate away from the constraint. Then, the linearized feasible directions are the whole plane!!! Which is not true. If we move even a tiny bit off the constraint (which proceeds in a linear direction), we are in a no-good situation!
	- Constraint qualifications ensure that these two are the same. Basically, the constraint qualifications tell us that for every active constraint, we get a single constraint vector which is linearly independent to the other constraint vectors. In a word
		- Linear independence constraint qualification (LICQ) holds if the set of active constraint gradients $\{ \nabla c_{i}(x), i \in \mathcal{A}(x) \}$ is linearly independent.
### 12.4 KKT Proof

#### Showing that the linear independence constraint qualification makes the linear feasible directions match the tangent cone.
First, we seek to prove that if the LICQ conditions hold, then we necessarily have that $T_{\Omega}(x^{*}) = \mathcal{F}(x^{*})$, though we always have $T_{\Omega}(x^{*}) \subset \mathcal{F}(x^{*})$.
- For the T subset F we use the given $z_{k}$ to compute, with Taylor series, the gradient at the point $x^{*}$.
- Implicit function theorem: if a function $f(x, y)$ is non-singular at a point $x=z^{*}, y = 0$, that is, it "looks" like an invertible linear function near that point. In other words, the Jacobian has a determinant not equal to zero... invertible. In that case, there is a neighborhood around $x$ and $y$ such that there is some function $x = h(y)$ so that $f(h(y), y) = 0$ everywhere in that neighborhood.
- How do we use the implicit function theorem? We choose $d \in \mathcal{F}(x^{*})$, and an arbitrary sequence $\{ t_{k} \}_{k=0}^{\infty}$ such that $\lim_{ k \to \infty } t_{k} = 0$. We are seeking $\{ z_{k} \}_{k}^{\infty}$ such that $\lim_{ k \to \infty } \frac{z_{k} - x^{*}}{t_{k}} = d$, so that $d \in T_{\Omega}(x^{*})$.
	- We consider $\mathcal{A}(x)$. Let $Z$ be the matrix made up of the nullspace of $\mathcal{A}$. Then, consider the following function:$$
R(z, t) = \begin{bmatrix}
c(z) - t \mathcal{A}(x^{*})d \\
Z^{T}(z - x^{*} - td)
\end{bmatrix}
$$
- Consider that this function is $0$ at our desired points! So when it is zero, we are in the space generated by the gradients (row space of $\mathcal{A}(x)$), since we are orthogonal to the nullspace of the matrix of rows of the gradients (which is precisely a basis of the vectors orthogonal to the gradients). Thus, there is some neighborhood of $(x^{*}, 0)$ where there is an implicit function $z(t)$ such that $R(z(t), t) = 0$  always holds. On such neighborhood (which we are guaranteed $t_{k}$ to converge inside) we can then take the unique $z_{k}$ from the implicit function $z(t), t$.
- Then, $z_{k}$ is such a desired sequence!
- If we don't have $\mathcal{A}(x^{*})$ to be of the expected rank, then $c(z) - t \mathcal{A}(x^{*}) d$ will not have, as expected, zero (non-singular) close to the point, and so we cannot apply the theorem! This is where stuff breaks down.
Next, we have as a necessary condition at a local optimum, that any $d \in T_{\Omega}(x^{*})$ must have $\nabla f(x^{*}) d \geq 0$.

#### Farkas Lemma
We define cones as $K = \{ By + Cw | y \geq 0 \}$. In particular, we fill some $k$ dimensional subspace, where $k$ is the dimension of the row space of $C$. Then, the cone defined by $By$ gets swept over this subspace.
To prove, we need to find some normal vector $d$ which satisfies our requirements. Take it to be the vector produced by the closest point $s$ and $g$, so it is the vector going from $g$ to $s$, that is $s - g$. We can see, that this will then produce a plane flush up against the cone, behind which $g$ lies, and in front of which the rest of the cone lies (otherwise, we won't have that $s$ is the closest point).

#### Final proof
Consider the following cone:
$$
N = \left\{ \sum_{ i \in \mathcal{A}(x^{*})} \lambda_{i} \nabla c_{i}(x^{*}), \lambda_{i} \geq 0 \text{ for } i \in \mathcal{A}(x^{*}) \cap \mathcal{I}  \right\}
$$
Then, either $\nabla f(x^{*})$ is in this cone, or we have some direction $d$ such that $d^{T} \nabla f(x^{*}) < 0$, and $d \in \mathcal{F}(x^{*})$ (the cone).
In particular, if $\nabla f(x^{*}) \not\in \mathcal{F}(x^{*})$, then we must have some $d \in \mathcal{F}(x^{*})$ which gives us this decreasing condition. However, this cannot be the case if we are at a local minimum.
Therefore for all of the $d \in \mathcal{F}(x^{*})$, we have $d^{T} f(x^{*}) \geq 0$, so we must have that $f(x^{*}) \in \mathcal{F}(x^{*})$ by Farka's lemma. The claim is that, if $x^{*}$ is a local solution, then all of this math plops out the KKT conditions. Do some other time.


### 12.5 Second Order Conditions
The Legrangian must satisfy $w^{T} \nabla_{\times} \mathcal{L}(x^{*}, \gamma^{*})w \geq 0$ for all $w \in \mathcal{C}(x^{*}, \gamma^{*})$, where $\mathcal{C}$ is the critical cone, at a local minimum. The critical cone is the set of feasible direction such that $\nabla f \cdot w = 0$  (in other words, it's either at the top of a hump). This is a necessary condition. A sufficient condition in this case, is that it holds strictly for all $w \in \mathcal{C}(x^{*}, \gamma^{*})$.

When we have the strict complementary condition, and also the LICQ condition, then we can consider that $\mathcal{C}(x^{*}, \gamma^{*}) = \text{Null}[\nabla c_{i}(x^{*})^{T}]_{i \in \mathcal{A}(x^{*})} = \text{Null}A(x^{*})$.
But we consider the same matrix, $Z$, which is defined as a matrix whose range is this nullspace, which is just the feasible directions, and then consider that the conditions then amount to saying $Z^{T} \nabla_{xx}^{2} \mathcal{L}(x^{*}, \gamma^{*})Z$ is positive semi-definite for the necessary condition, and positive definite for the sufficient condition.

# 16 Quadratic Programming
- We begin with an example: suppose we are trying to invest a bunch of money. We consider the sum of our investments as the random variable. The variance will be a quadratic w.r.t. the covariance matrix, and the expected money will be the sum of the expectation. However, we can then consider this as a quadratic problem: increase expected return, decrease expected variance.
- Now, we apply the KKT conditions to the QP problem, to get some nice linear solutions. The KKT matrix is given by $$ \begin{bmatrix}
G & A^{T}  \\
A  & 0
\end{bmatrix} \begin{bmatrix}
-p  \\
\lambda^{*}
\end{bmatrix} = \begin{bmatrix}
g \\
h
\end{bmatrix} $$
	- Here, $h = Ax - b$, $g = c + Gx$, and $p = x^{*} - x$ (that is, this matrix describes the solution to a step $p$ to get to some estimate of the solution $x$ to the optimal solution $x^{*}$) or, alternatively, when $x = 0$, it simply describes the equations satisfied at our optimal point. Also, we are minimizing $x^{T}Gx + c^{T}x$ constrained to $Ax = b$.
- $Z^{T}GZ$ positive definite is the case when $G$ is strictly convex. Otherwise, we are positive semidefinite. When it is positive definite, then we can easily prove with linear algebra that there exists a unique solution to the KKT matrix. We can also prove from the problem itself that said solution is the global solution in that case, (which in a way follows from the first order conditions).

Next week: figure out how KKT systems work!

## 16.3
- We show 3 different ways we can solve the system.
	- The first is essentially a useful matrix decomposition. The first noteworthy thing, is that before performing Gaussian elimination, it is profitable to order the rows such that, when you divide through by the coefficient, you aren't dividing by some impossibly small value. It's a generalized L^T L (positive square root) factorization, that works even when you don't have positive square roots (the matrix will always be positive, since $G$ is the hessian of the problem.)
	- The second is essentially computing block Gaussian elimination with the inverse of $G$, and then solving first for $\lambda$ using the lower rows, and then solving for the upper rows.
	- Finally, the null space method breaks up $p$ into two spaces, $Z$ and $Y$, which are each individually easier to solve and breaks us up into individual equations.

## 18 Sequential Quadratic Programming

First, we apply Newton's method to the KKT conditions. In the quadratic case, we could simply solve for the location when $\nabla \mathcal{L}$ was zero, since this derivative was a linear system, but for a general function $f$, we need to use newton's method with this. The idea is, to use newton's method on this first derivative, so we get to a second derivative. Suppose we wanted to solve for the minimum of a quadratic. We could take the derivative at our starting point, and using the second derivative to see how far we need to move to get our derivative to be zero, at which point we know we are at $x = 0$.
A second way we can think about formulating this problem is by approximating it by a quadratic in the first place $$
\min_{p} f_{k} + \nabla f_{k}^{T} p + \frac{1}{2} p^{T} \nabla^{2}_{xx} \mathcal{L}_{k} p
$$
and then simply going to the optimal solution to this problem right away. We use the hessian of the Lagrangian instead of $f$ so that we are accounting for the curvature along the constraint.
Since $\nabla f = \lambda \nabla g$, we have that the next derivative, actually gives us an accurate indication of how much the curvature of $g$ will change our result (since it is scaled by $\lambda$ to tell us what it's contribution will be to the value of $f$).

On inequality problems: If, at each iteration, we are solving the SQP problem, eventually, we will get close enough so that our active set matches the active set of the nonlinear program at $x^{*}$, provably. Chapter 16 details ways that inequality constraints are dealt with: we step along the constraints in the direction that we are supposed to go in until one of the new constraints are met.

## 18.2 Preview of Practical SQP methods
- We may solve the quadratic program completely, iterating the working set each step.
- We may fix the working sep and solve the quadratic program at each step completely, and then continue to update the working set based on what we see fit.
### Enforcing convergence
- If we are close to the point we want to get to, we may have a positive definite hessian that will result in a nice convergence, but this might not be case if we are farther away. Indeed, if we are negative definite, we can use trust region methods, which ensure we don't travel too far down an upside down bowl (before another region of the function becomes a right side up bowl for us to fall down into).
- In quasi-newton methods (line search)... TBC!

## 18.3 Algorithmic Development
First, we have to deal with the reality that often, linearizing the constraints will result in inconsistencies. We can replace hard constraints with new equations that add another variable that deals with how much the constraint is being violated, and then add these variables to the function we are trying to minimize, times some constant $\mu$, so that it is minimized as well. By choosing $\mu$ large enough we can then show that if a feasible solution exists we come there, and if not, then we come to the closest point to being feasible.

The BFGS method:
- If we cannot compute the hessian, we can construct a second derivative matrix by creating the matrix that is closest to the previous iteration that matches the derivative at the current point and the previous point. This is called the BFGS method... however, in order to ensure that this second derivative is positive definite, we require that the partial derivative be increasing in the direction that we are minimizing (curving up as we go downhill), which is expressed in the condition $s_{k}^{T}y_{k} \geq 0$.
- If the hessian of the Lagrangian is not positive definite, we can use an interpolation between the previous iteration and the current iteration that ensures that we are still positive definite but still moving towards a more accurate solution. (This is when we are iterating the quadratic, in order to determine our gradient steps).
- Next, we consider the "reduced hessian"... the idea is, the update $p_{k}$ has the component in the range of $A_{k}$ updated such that the constraints are matched, and the component in the nullspace of $A_{k}$ is determined completely by the hessian, so by projecting onto the nullspace of $A_{k}$ we have the so-called "Reduced Hessian". Then, $p_{k}$ and $\lambda_{k}$ can be solved separately. Next, we split $p_{k}$ into the components in the range of $A_{k}$ and in the nullspace, and then argue that we can drop certain terms in the equations, depending on which ones are relevant for both the $\lambda$ and $p$ equations. After simplifying, we can use Hessian approximation methods in order to finish the job!
- Next, we deal with merit functions. Merit functions help us compute inequalities by adjusting our objective function with a penalty. We have a theorem that helps us understand what scalar multiple we need on the penalty in order to ensure that we are actually decreasing the objective function/getting closer to our constraints. There are a couple of heuristics for choosing the scalar, so we don't make too little progress.
- In essence, we want $\mu$ to be the ratio between how much we are expecting the objective function's value to change and the constraint. We want them to be proportional, and $\mu$ is in some sense a forcing term that ensures that once the constraint becomes small, it's "force" in the optimization problem is similar to that of the change in the objective function. By approximating the change in the objective function quadratically, we can then derive an expression $$
\mu \geq \frac{\nabla f_{k}^{T}p_{k} + (\sigma / 2) p_{k}^{T} \nabla^{2}_{x x} \mathcal{L}_{k} p_{k}}{(1-\rho) \lvert c_{k} \rvert_{1}}
$$, where $\rho$ and $\sigma$ are other parameters we can adjust.

We put all of these pieces together, in order to create our first algorithms!
### 18.5 Trust Region Methods
The big problem with line search methods: they use **newton's method to set their sights on a critical point**, regardless of whether this is a saddle point, local maximum, or local minimum: positive definiteness is actually required if we want to make sure we are really moving downward. Trust region methods, instead, **fix the maximum distance** we are allowed to move, and then search for a minimum within that region. The problem is, we may have to **find some way to deal with the constraints** in the case, since there may be no points where the constraints are satisfied within the trust region. There are three such methods we can discuss.
#### Relaxation Methods
We change the constraint by the smallest vector that will allow us to be in the trust region. We compute this vector by solving another minimization problem.

#### S$\ell_{1}$QP (Sequential $\ell_{1}$ Quadratic Programming)
